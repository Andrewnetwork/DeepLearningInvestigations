{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Autoencoder Experiments\n",
    "** March 2018 **\n",
    "\n",
    "** Andrew Riberio @ [AndrewRib.com](http://www.andrewrib.com) **\n",
    "\n",
    "Resources\n",
    "* https://www.tensorflow.org/get_started/mnist/beginners\n",
    "* https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andre\\Anaconda3\\envs\\GreatEnv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import losses\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from ipywidgets import interact_manual,interact\n",
    "import pandas as pd\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Start tensorflow session and have keras use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True,device_count = {'CPU' : 1, 'GPU' : 1}))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering and Working with MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Loading the dataset object into the mnist variable and printing the dimensions of the data matrix and label vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Shape of the image data matrix: (55000, 784)\n",
      "Shape of the label data: (55000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "print(\"Shape of the image data matrix: {0}\".format(mnist.train.images.shape))\n",
    "print(\"Shape of the label data: {0}\".format(mnist.train.labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2.2. Visualizing an image in our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB5Ii/EBShB9IivADSRF+IKn/BVqv6fl+iggtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a214097f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image label: 6\n"
     ]
    }
   ],
   "source": [
    "exampleIndex = 3\n",
    "exImage      = mnist.train.images[exampleIndex ,:]\n",
    "exImageLabel = mnist.train.labels[exampleIndex]\n",
    "plt.imshow(exImage.reshape(28,28),cmap='gray')\n",
    "plt.show()\n",
    "print(\"Image label: {0}\".format(exImageLabel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Split images into digit class tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0 matrix shape: (5444, 784)\n",
      "Digit 1 matrix shape: (6179, 784)\n",
      "Digit 2 matrix shape: (5470, 784)\n",
      "Digit 3 matrix shape: (5638, 784)\n",
      "Digit 4 matrix shape: (5307, 784)\n",
      "Digit 5 matrix shape: (4987, 784)\n",
      "Digit 6 matrix shape: (5417, 784)\n",
      "Digit 7 matrix shape: (5715, 784)\n",
      "Digit 8 matrix shape: (5389, 784)\n",
      "Digit 9 matrix shape: (5454, 784)\n"
     ]
    }
   ],
   "source": [
    "# First we will zip the training labels with the training images\n",
    "dataWithLabels = zip(mnist.train.labels, mnist.train.images)\n",
    "\n",
    "# Now let's turn this into a dictionary where subsets of the images in respect \n",
    "# to digit class are stored via the corresponding key.\n",
    "\n",
    "# Init dataDict with keys [0,9] and empty lists.\n",
    "digitDict = {}\n",
    "for i in range(0,10):\n",
    "    digitDict[i] = []\n",
    "\n",
    "# Assign a list of image vectors to each corresponding digit class index. \n",
    "for i in dataWithLabels:\n",
    "    digitDict[i[0]].append(i[1])\n",
    "\n",
    "# Convert the lists into numpy matricies. (could be done above, but I claim ignorace)\n",
    "for i in range(0,10):\n",
    "    digitDict[i] = np.matrix(digitDict[i])\n",
    "    print(\"Digit {0} matrix shape: {1}\".format(i,digitDict[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Anomoly Detection for Non-Zero Class Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Compile a simple single layer non-convolutional autoencoder for the digit class 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleAE(encoding_dim = 32, input_dim = 784):\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    return Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae1 = simpleAE(64,784)\n",
    "ae1.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Train the autoencoder and log the loss over epoch history. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model found and loaded.\n"
     ]
    }
   ],
   "source": [
    "modelURI = \"models/mnist_zero_autoencoder_1_64.h5\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "\n",
    "try:\n",
    "    # Load model if found. \n",
    "    ae1 = load_model(modelURI)\n",
    "    print(\"Model found and loaded.\")\n",
    "except:\n",
    "    # Train model if model cannot be loaded. \n",
    "    print(\"Model not found. Training model..\")\n",
    "    \n",
    "    history = ae1.fit(digitDict[0], digitDict[0],\n",
    "                    epochs=100,\n",
    "                    batch_size=20,\n",
    "                    shuffle=True,verbose=1)\n",
    "    \n",
    "    # Save our Model\n",
    "    print(\"Model Saved.\")\n",
    "    ae1.save(modelURI)\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Explore trained autoencoder reconstruction results ( Interactive )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5a185717db4bf799d17aa1e2b5bcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='digitClass', max=9), IntSlider(value=0, description='nthDigit', max=1000), Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dispRes1>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dispRes1(digitClass=0,nthDigit=0):\n",
    "    actual = digitDict[digitClass][nthDigit,:]\n",
    "    pred = ae1.predict(actual)\n",
    "    res = losses.mean_absolute_error(actual,pred)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(\"Mean Absolute Reconstruction Error: {0:.5}%\".format(sess.run(res)[0]*100), fontsize=16,y=0.73)\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax1.imshow(actual.reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Reconstructed Image\")\n",
    "    ax2.imshow(pred.reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "interact_manual(dispRes1,digitClass=(0,9),nthDigit=(0,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Reconstruction Error Distributions ( Interactive )\n",
    "We will now explore the reconstruction error distributions over the digit classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classLosses = []\n",
    "for digitClass in range(0,10):\n",
    "    lossTensor = losses.mean_absolute_error(digitDict[digitClass],ae1.predict(digitDict[digitClass]))\n",
    "    res = sess.run(lossTensor)\n",
    "    res.sort()\n",
    "    classLosses.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1b2bd7d1984e75ad325357f07205cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='digitClass', max=9), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dispDists>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dispDists(digitClass=0):\n",
    "    l = len(classLosses[digitClass])\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.hist(classLosses[digitClass],bins=20,range=(0, 0.125))\n",
    "    plt.show()\n",
    "    \n",
    "interact(dispDists,digitClass=(0,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now plot the histograms of the reconstruction error distributions over the digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAJCCAYAAABnD3vtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a21e845908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for digitClass in range(0,10):\n",
    "    l = len(classLosses[digitClass])\n",
    "    plt.legend(['0','1','2','3','4','5','6','7','8','9'], loc='upper right')\n",
    "    plt.hist(classLosses[digitClass],bins=20,range=(0, 0.125))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5444.000000</td>\n",
       "      <td>6179.000000</td>\n",
       "      <td>5470.000000</td>\n",
       "      <td>5638.000000</td>\n",
       "      <td>5307.000000</td>\n",
       "      <td>4987.000000</td>\n",
       "      <td>5417.000000</td>\n",
       "      <td>5715.000000</td>\n",
       "      <td>5389.000000</td>\n",
       "      <td>5454.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.023555</td>\n",
       "      <td>0.050618</td>\n",
       "      <td>0.060406</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.064543</td>\n",
       "      <td>0.055918</td>\n",
       "      <td>0.048636</td>\n",
       "      <td>0.054238</td>\n",
       "      <td>0.059698</td>\n",
       "      <td>0.059719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.011172</td>\n",
       "      <td>0.012814</td>\n",
       "      <td>0.013837</td>\n",
       "      <td>0.011656</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.011189</td>\n",
       "      <td>0.012223</td>\n",
       "      <td>0.014361</td>\n",
       "      <td>0.010814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.011773</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.021558</td>\n",
       "      <td>0.031291</td>\n",
       "      <td>0.024523</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>0.025370</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.025546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.019330</td>\n",
       "      <td>0.042684</td>\n",
       "      <td>0.051233</td>\n",
       "      <td>0.042509</td>\n",
       "      <td>0.057116</td>\n",
       "      <td>0.047245</td>\n",
       "      <td>0.040849</td>\n",
       "      <td>0.045301</td>\n",
       "      <td>0.049665</td>\n",
       "      <td>0.052791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.022452</td>\n",
       "      <td>0.050653</td>\n",
       "      <td>0.060306</td>\n",
       "      <td>0.050756</td>\n",
       "      <td>0.063328</td>\n",
       "      <td>0.054275</td>\n",
       "      <td>0.047860</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.058125</td>\n",
       "      <td>0.058717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.026556</td>\n",
       "      <td>0.057630</td>\n",
       "      <td>0.069113</td>\n",
       "      <td>0.060987</td>\n",
       "      <td>0.070706</td>\n",
       "      <td>0.062614</td>\n",
       "      <td>0.055148</td>\n",
       "      <td>0.061687</td>\n",
       "      <td>0.067644</td>\n",
       "      <td>0.065478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.060401</td>\n",
       "      <td>0.113417</td>\n",
       "      <td>0.114180</td>\n",
       "      <td>0.113039</td>\n",
       "      <td>0.123094</td>\n",
       "      <td>0.120863</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>0.109914</td>\n",
       "      <td>0.130665</td>\n",
       "      <td>0.120293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2            3            4  \\\n",
       "count  5444.000000  6179.000000  5470.000000  5638.000000  5307.000000   \n",
       "mean      0.023555     0.050618     0.060406     0.052698     0.064543   \n",
       "std       0.006048     0.011172     0.012814     0.013837     0.011656   \n",
       "min       0.011773     0.014641     0.022195     0.021558     0.031291   \n",
       "25%       0.019330     0.042684     0.051233     0.042509     0.057116   \n",
       "50%       0.022452     0.050653     0.060306     0.050756     0.063328   \n",
       "75%       0.026556     0.057630     0.069113     0.060987     0.070706   \n",
       "max       0.060401     0.113417     0.114180     0.113039     0.123094   \n",
       "\n",
       "                 5            6            7            8            9  \n",
       "count  4987.000000  5417.000000  5715.000000  5389.000000  5454.000000  \n",
       "mean      0.055918     0.048636     0.054238     0.059698     0.059719  \n",
       "std       0.012653     0.011189     0.012223     0.014361     0.010814  \n",
       "min       0.024523     0.017317     0.025370     0.020200     0.025546  \n",
       "25%       0.047245     0.040849     0.045301     0.049665     0.052791  \n",
       "50%       0.054275     0.047860     0.052630     0.058125     0.058717  \n",
       "75%       0.062614     0.055148     0.061687     0.067644     0.065478  \n",
       "max       0.120863     0.103828     0.109914     0.130665     0.120293  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary Statistics for Digit Reconstruction Loss\n",
    "pd.DataFrame(classLosses).transpose().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. Classify reconstruction loss. \n",
    "**3.5.1. Create loss classifier network architecture **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleNN(encoding_dim = 10, input_dim = 1):\n",
    "\n",
    "    # this is our input placeholder\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    # \"encoded\" is the encoded representation of the input\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    return Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.2. Preprocess data for classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Data for Classifier\n",
    "zeroClass = classLosses[0]\n",
    "otherClass = np.concatenate(classLosses[1:10])\n",
    "labels = np.concatenate([np.zeros(len(zeroClass)),np.ones(len(otherClass))])\n",
    "data = np.concatenate([zeroClass,otherClass])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.3. Train loss classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model found and loaded.\n"
     ]
    }
   ],
   "source": [
    "modelURI = \"models/reconstruction_loss_1_64.h5\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "\n",
    "try:\n",
    "    # Load model if found. \n",
    "    snn = load_model(modelURI)\n",
    "    print(\"Model found and loaded.\")\n",
    "except:\n",
    "    # Train model if model cannot be loaded. \n",
    "    print(\"Model not found. Training model..\")\n",
    "    \n",
    "    snn = simpleNN()\n",
    "    snn.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    history = snn.fit(data, labels,\n",
    "                        epochs=25,\n",
    "                        batch_size=20,\n",
    "                        shuffle=True,verbose=1)\n",
    "    \n",
    "    # Save our Model\n",
    "    print(\"Model Saved.\")\n",
    "    snn.save(modelURI)\n",
    "    \n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.4. Visualize results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45052f7c946747ddbc35e4fdade3189a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='digitClass', max=9), IntSlider(value=0, description='nthDigit', max=1000), Button(description='Run Interact', style=ButtonStyle()), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.dispRes1>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dispRes1(digitClass=0,nthDigit=0):\n",
    "    classificationThresh = 0.5\n",
    "    actual = digitDict[digitClass][nthDigit,:]\n",
    "    pred = ae1.predict(actual)\n",
    "    res = losses.mean_absolute_error(actual,pred)\n",
    "    res = sess.run(res)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(\"Mean Absolute Reconstruction Error: {0:.5}%\".format(res[0]*100), fontsize=16,y=0.73)\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax1.imshow(actual.reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Reconstructed Image\")\n",
    "    ax2.imshow(pred.reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    snnRes = snn.predict(res)[0][0]\n",
    "    \n",
    "    if snnRes < classificationThresh:\n",
    "        print(\"Classification 0 : [{0}]\".format( 1-snnRes ))\n",
    "    else:\n",
    "        print(\"Classification ANOMALY : [{0}]\".format( snnRes ))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "interact_manual(dispRes1,digitClass=(0,9),nthDigit=(0,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.5. Evaluate the classification accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationThresh = 0.5\n",
    "\n",
    "# Get all the training images. \n",
    "actual = mnist.train.images\n",
    "\n",
    "# Pass all the images through the trained zero digit autoencoder. \n",
    "pred = ae1.predict(actual)\n",
    "\n",
    "# Calculuate the reconstruction loss for each image passed through the autoencoder. \n",
    "res = losses.mean_absolute_error(actual,pred)\n",
    "\n",
    "# Use tensorflow to compute the value. \n",
    "res = sess.run(res)\n",
    "\n",
    "# Pass the losses through the loss classifier. \n",
    "snnRes = snn.predict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tresholdFn( loss ):\n",
    "    if loss < classificationThresh:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "predictions = np.vectorize(tresholdFn)(snnRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false negatives: 554\n",
      "Percentage of false negatives: 10.2%\n",
      "---\n",
      "Number of false positives: 684\n",
      "Percentage of false positives: 1.38026%\n"
     ]
    }
   ],
   "source": [
    "nData = mnist.train.labels.shape[0]\n",
    "nFalsePos = 0\n",
    "falsePosIdxs = []\n",
    "nFalseNeg = 0\n",
    "falseNegIdxs = []\n",
    "\n",
    "for index,prediction,label,in zip(range(nData),predictions,mnist.train.labels):\n",
    "    # Compute the number of false negatives. \n",
    "    if label == 0 and prediction == 1:\n",
    "        nFalseNeg += 1\n",
    "        falseNegIdxs.append(index)\n",
    "        \n",
    "    # Compute the number of false positives. \n",
    "    if label != 0 and prediction == 0:\n",
    "        nFalsePos += 1\n",
    "        falsePosIdxs.append(index)\n",
    "    \n",
    "print(\"Number of false negatives: {0}\".format(nFalseNeg))\n",
    "print(\"Percentage of false negatives: {0:.3}%\".format(nFalseNeg/digitDict[0].shape[0]*100))\n",
    "print(\"---\")\n",
    "print(\"Number of false positives: {0}\".format(nFalsePos))\n",
    "print(\"Percentage of false positives: {0:.6}%\".format(nFalsePos/otherClass.shape[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoResults = ae1.predict(mnist.train.images)\n",
    "lossRes = losses.mean_absolute_error(mnist.train.images,autoResults)\n",
    "lossRes = sess.run(lossRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize False Positives. False positives are non-zero characters that are classified as zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f520d046df4b1eb7d82d39a874c899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx', max=683), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.displayFalsePos>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def displayFalsePos(idx = 0):\n",
    "    classificationThresh = 0.5\n",
    "    \n",
    "    indexVis = falsePosIdxs[idx]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(\"False Positives - Mean Absolute Reconstruction Error: {0:.5}%\".format(lossRes[indexVis]*100), fontsize=16,y=0.73)\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax1.imshow(mnist.train.images[indexVis,:].reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Reconstructed Image\")\n",
    "    ax2.imshow(autoResults[indexVis,:].reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Absolute index value: {0}.\".format(indexVis))\n",
    "    print(\"Image label: {0}.\".format(mnist.train.labels[indexVis]))\n",
    "        \n",
    "interact(displayFalsePos,idx = (0,len(falsePosIdxs)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39262c1fa5ed4838802bf2046e0c3309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx', max=553), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.displayFalseNeg>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def displayFalseNeg(idx = 0):\n",
    "    classificationThresh = 0.5\n",
    "    \n",
    "    indexVis = falseNegIdxs[idx]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fig.suptitle(\"False Negatives - Mean Absolute Reconstruction Error: {0:.5}%\".format(lossRes[indexVis]*100), fontsize=16,y=0.73)\n",
    "    \n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax1.imshow(mnist.train.images[indexVis,:].reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title(\"Reconstructed Image\")\n",
    "    ax2.imshow(autoResults[indexVis,:].reshape(28,28),cmap=\"gray\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Absolute index value: {0}.\".format(indexVis))\n",
    "    print(\"Image label: {0}.\".format(mnist.train.labels[indexVis]))\n",
    "        \n",
    "interact(displayFalseNeg,idx = (0,len(falseNegIdxs)-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of false negatives: 757\n",
    "Percentage of false negatives: 13.9%\n",
    "---\n",
    "Number of false positives: 394\n",
    "Percentage of false positives: 0.79506%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
